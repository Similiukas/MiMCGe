\documentclass{Resources/UoBLab1}

% theorems and stuff https://www.overleaf.com/learn/latex/Theorems_and_proofs
\usepackage{amsthm}
\usepackage{etoolbox}
% \usepackage{parskip}
% \usepackage{indentfirst}    % To indent first paragraph after section starts
\theoremstyle{definition}
% theorem numbering: https://tex.stackexchange.com/questions/45817/theorem-definition-lemma-problem-numbering
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% ignoring indetation after definition/theorem and stuff https://tex.stackexchange.com/questions/176027/no-indentation-after-theorem-environment-with-amsthm
% \AfterEndEnvironment{definition}{\noindent\ignorespaces}

% hyper links: https://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
% \usepackage{cite}
\usepackage{xcolor} % hyperref color stuff
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}
% ----- Figure drawing packages
\usepackage{tikz}
%%% Public TikZ libraries
\usetikzlibrary{positioning}

%%% Custom TikZ addons
% \usetikzlibrary{crypto.symbols}
\usetikzlibrary{crypto.symbols}
%\tikzset{shadows=no}        % Option: add shadows to XOR, ADD, etc.
\usepackage{pgfplots}
% -----------------

% To name the algorithm package
\usepackage{algorithm}
% Pseudocode pacakge
\usepackage{algpseudocode}

\usepackage{xlop}

\pubyear{2023}
\subjectarea{Computer Science Project}
\begin{document}
\firstpage{1}

\title{Try different polynomials in the structure of MiMC and analyse efficiency, and basic cryptographic properties.}
\author{Simonas Tautvaišas 2204062}
\course{M.Sci. Mathematics and Computer Science}
\supervisor{Dr Rishiraj Bhattacharyya}
\school{School of Mathematics}
\date{2023-12-16}

\maketitle

% check references URLs
% spell check su grammarly
% pakeisti degree is M.Sci.
% Maybe also have a table of contents?
% Paziureti "kabutes" ar tikrai taip reikia jas deti
% Galbut paziureti "we" ir bandyti perfrazuoti
% not everything is a theorem, there are lemmas as well
% Dar reiktų duoti formal definition of what a cipher is. Google it, chat it, bet ir tiesiog funkcija of pair, with inverses
% Galima parašyt, kad su x^16 diffusion 100 ir 10000 nesiskiria
% Give proper definition to cipher with figure refer ir su pavadinimu mimcgn
% Perziuret cites, ar netycia nesikartoja
% Reiketu dar kazkur paekcentuoti, kad cia nieko naujo nedarom, bet tik introduction, kaip ciphers are proving to be secure
% Galima parasyt, kad project required to learn a lot of cryptanalysis from scratch by myself. Ir sellint, kad ner taip lengva. Galima duoti reference i cryptanalysis course material of same people who designed and broke MiMC.
% https://stackoverflow.com/a/10548919/9819103
% Nuspresti captions of figures apacioj ar virsuj
% reikia student id ir word count in front page
% Duoti CPU, kai testuojamas laikas

words: 632/981/1932/4345/5891/8032

\begin{abstract}
    Talk mostly about what was in the proposal but simplified.
\end{abstract}

% ------------- INTRODUCTION ----------------
\section{Introduction}
\begin{itemize}
    \item Talk about modern cryptography and explain SNARK, MPC, ZK
   \item What previously has been done related to this
    \item What we are going to do
    \begin{itemize}
        \item Take new polynomial
        \item Prove that it's still secure
        \item Works for ZK, MPC?
        \item Performance check
    \end{itemize}
\end{itemize}
\vspace{15pt}
In modern day cryptography there are many new techniques that focus on problems beyond classic confidentiality and authenticity between two-party communications. Research ideas like zero-knowledge proofs (ZK), secure multi-party computation (MPC) or fully homomorphic encryption (FHE) have grown in popularity over recent years. A large class of applications that use these ideas have cryptographic schemes that focus on minimizing the number of multiplications. Primarily, there are algorithms that work in a specific field of $\mathbb{F}_p$ or $\operatorname{GF}(p)$ for a prime $p \geq 3$. Working in a Galois field gives a few unique properties since numbers are in a ring structure. Some examples of cryptographic functions which capitalize these properties to lower number of multiplications include GMiMC\cite{GMiMC}, Poseidon\cite{Poseidon}, Griffin\cite{Griffin} and MiMC\cite{MiMC}.

The goal to lower the number of multiplications comes from the fact that these algorithms are specifically intended to be optimized for ZK, MPC and FHE. Consider the current block cipher standard AES which with modern hardware equipped with CPU specific instruction sets (AES-NI) can process close to 1GB/s\cite{AESNI} of data. However in MPC context, this number is drastically lower which limits the usage of AES in multi-party computation protocols\cite{AESInMPC} \textcolor{orange}{Nesu tikras del to cite. Tikras is LowMC 2nd page, 2nd par}. This comes from the fact that traditionally ciphers are built from linear and non-linear blocks which have similar costs in hardware and software. However, in the MPC or FHE context, linear operations, which can be simply viewed as XOR operations rather than non-linear AND operations, are way cheaper and are essentially free since they only incur local computation. Thus the need for ciphers which optimize low multiplicative complexity arises\textcolor{red}{should reread and see if it fits with subsection 1.1 and doesn't repeat same things}.

\subsection{Related work}
\textbf{Secure multi-party computation.} The usage of cloud computing over the past decade has grown immensely\textcolor{orange}{butu gerai rasti souce, bet vis roda future trends} and it is predicted that by 2028 more than 50\% of businesses will use it\cite{CloudTrend}. Thus, one of cloud computing needs is for devices to jointly compute some result and be assured that the information is private and correct. This is the goal of secure multi-party computation (MPC) first introduced by Goldreich, Micali and Wigderson\cite{MPCFirst}. One other usage of MPC, for example, could be to check person's DNA against a database of cancer patients' DNA, to see if the person is in high risk group for certain cancer type. Clearly, DNA is a highly private information hence should not be revealed, however, using MPC, we can solve this issue guaranteeing privacy.
\textcolor{blue}{gal parasyti kokius cipher ar kazka praktisko, kas yra su tuo MPC.}

MPC is not just a theoretical concept as it has been successfully used in practice. One such example is in Estonia where a privacy preserving study has been conducted using MPC framework Sharemind\cite{Sharemind}. Encrypted income tax together with higher education records have been collected to look for a correlation between working during studies and failing to graduate.\cite{Estija}. One of the key aspects which is fundamental in MPC is the secret sharing which allows multiple parties to share a secret in a way no single party is able to reconstruct the secret on their own. Various secret sharing schemes are used, such as Shamir's Secret Sharing\cite{Shamir}. Another fundamental aspect in MPC is correctness, which provides evidence that neither of the parties cheat or manipulate the data. This is ensured using Zero-Knowledge proofs.\medskip

\noindent\textbf{Zero-Knowledge.} The term Zero-Knowledge (ZK) proofs were first introduced by Goldwasser, Micali and Rackoff\cite{ZKOrigin} where they give a great example of what it is: to prove that a graph is Hamiltonian it suffices to give a Hamiltonian path, however this rather gives more information than just the single bit whether the graph is Hamiltonian or not. Thus, loosely speaking, Zero-Knowledge proofs yield nothing more than just the validity of the assertion.

% https://www.reddit.com/r/LaTeX/comments/gqcqug/if_underfulll_hbox_badness_10000_in_paragraph_is/
There are many flavours in which these proofs come depending on the application. The choice depends on the desired security and performance properties. For example, considering security settings, most protocols depend on a number of assumptions. They are in computational nature, where some mathematical problem is assumed to be hard to solve. This can be an assumption that a hash function behaves like a truly random function.\cite{RNGZK}. There are also interactive and non-interactive proofs. For example, we can interactively prove graph isomorphism\cite{ZKProofSystemsBook}. Interactive proofs are such where the prover and verifier interact over multiple rounds and can change responses based on the already received messages. However, what we are more interested in are non-interactive Zero-Knowledge proofs (NIZK) proposed by Blum, Feldman and Micali\cite{NIZK}.\medskip

\noindent\textbf{Fully Homomorphic Encryption.} One such NIZK is the Boneh-Goh-Nissim\cite{BGNCipher} encryption which is somewhat homomorphic with respect to addition and multiplication respectively. Homomorphic means that computation and analysis can be performed on encrypted data without first decrypting it. There are different levels of homomorphisms from the simplest \textit{partially} homomorphic, \textit{somewhat} homomorphic to \textit{fully} homomorphic encryption (FHE). This level of homomorphism allows computation of data with arbitrary level of depth or complexity\cite{FHEPHD}. Note, that the first FHE cryptography system appeared only recently (2009\cite{FHEPHD}) even though it was first proposed over 45 years ago\cite{FHEMention} as privacy homomorphism.

Currently, it is one of the more active research areas in cryptography, as its' applications are invaluable to modern day computing. It allows data to be computed by untrusted environment (such as cloud computing) without breaking confidentiality and preserving privacy concerns. At the moment, the main limitations is the increased computational degree, especially the multiplicative degree, however, works are made to make it more efficient and reduce the overhead when computing with such data\cite{FHEImprovement1}\cite{FHEImprovement2}.\textcolor{blue}{Maybe whether our cipher will be like that}\medskip

\noindent\textbf{SNARKs.} There is a problem with non-interactive proof systems when the proof is bigger than the statement itself. To deal with this, we need to make sure that the proofs are brief. If we take NIZK and add another property \textit{succinctness}, then we can also look at a succint non-interactive argument of knowledge (SNARK) systems. What succinctness means is that the proof is extremely efficient to verify and also much smaller than the statement itself.

One way this property is applied is through the usage of \textit{verifiable computation}, formalised by Gennaro, Gentry and Parno\cite{UntrustedWorkers}. They describe a system where a computationally weak client can outsource computationally heavy task to one or more workers, for example in cloud computing environment. Then the workers return the result together with a proof that such result is correct. SNARK guarantees that the verification of the proof is easier than doing the task itself. Thus, the client is assured they got the required result without doing heavy computations themselves.

One such almost practical implementation is Pinocchio\cite{Pinocchio}. Using this system, it is possible to produce a scheme to convince a client that the computation is correct. This is done using a compiler which converts C code into a format suitable for verification. It has been tested on gas simulations, image matching and computing SHA-1 and shown to be faster than native execution showing potential for zero-knowledge applications. Advancements have been to achieve practical implementations of zero knowledge proof systems such as Gepetto\cite{Gepetto}, MIRAGE\cite{Mirage}, Buffet\cite{Buffet} and others. One problem with these systems, is that they require a trusted setup\textcolor{orange}{what is trusted setup?}, however, there are also \textit{zero-knowledge Scalable Transparent ARguments of Knowledge} (zk-STARK) proof systems, introduced by Eli Ben-Sasso et al\cite{STARK}. They offer transparency which means anyone can verify the proof without needing any secret information. This is particularly useful in decentralised applications like blockchain. In fact, the first widespread use of zk-SNARK is in the implementation of zerocash blockchain\cite{zcash}.\medskip

\noindent\textbf{Bottlenecks.} In most of these scenarios for MPC, ZK and FHE, part of the circuit that is being evaluated is a pseudo-random number function (PRF), a collision resistant function or symmetric encryption. In a lot of these cases, the underlying bottleneck is the order of multiplications. Thus, research is done to create new ciphers and hash functions which circumvent this weakness. \textcolor{gray}{There are encryption schemes such as Ring-LWE\cite{RINGLWE} that offers lower multiplicative complexity in homomorphic scenarios compared to traditional number theoretic encryption schemes. Then there are CKKS\cite{CKKS}, BFV\cite{BFV1}\cite{BFV2} and even recent improvements to BFV\cite{BFVImprov} which improve Ring-LWE by leveraging low-degree polynomials to perform operations with reduced multiplicative degree.}. There is also the already mentioned cipher schemes Poseidon\cite{Poseidon}, MiMC\cite{MiMC}. There are also cipher families like Rescue together with Vision\cite{Rescue} which try to minimize arithmetic complexity by using simple round function and favouring addition over multiplication. These families are specifically intended to be used in zk-STARK and MPC scenarios.\textcolor{blue}{Kazkaip gal dar kazka su tuo parasyt ir graziau tiesiog?}
% Butu gerai ta bottleneck argumentuoti. Tiesiog paimti is vieno cipher (pvz rescue) ir kodel reikia mazo to mult degree. Galima remtis https://eprint.iacr.org/2021/267.pdf, nes graziai papasakoja apie bottlenecks and all

Even though the concepts for ZK, MPC, FHE have been introduced in the 70s and 80s, the first practical implementations are relatively recent. However, as the world transitions more and more into the digital cyberspace, use cases for these principles are now more important than ever. Privacy and data correctness plays a significant role in every day life. We want to make sure, that our sensitive personal data is handled properly and not leaked to any malicious third party. Thus, getting more efficient and better optimized fundamental encryption schemes allows for bigger widespread tool creation and usage which ensure these concepts.

\subsection{Our Goals}
In this project, we are going to focus on MiMC, which is constructed of the simplified round function of the Knudsen-Nyberg cipher from 1995\cite{KNCipher}. The core component of this algorithm is the use of \(f(x)=x^3\) APN (almost perfect nonlinear) function \textcolor{red}{add reference to what it is or explain it or both}. What we are going to do is \textit{build new ciphers based on the structure of MiMC by trying different polynomials to better understand how cryptographic algorithms are proven to be secure and then see whether our newly built cipher is efficient and what are the possible applications.}

Changing the core function of MiMC which is proven to be secure with a different polynomial is not enough to say that our new cipher is also secure. We are still required to prove that it holds key cryptographic properties like \textit{Diffusion}, \textit{Confussion}, \textit{Avalance Effect}, \textit{Key Sensitivity}, etc. \textcolor{red}{should probably say only the ones being tested}. We will use various methods to test these properties which include mathematical analysis, simulation, and empirical testing. \textcolor{red}{papildyti, apie testavimo budus}

After deciding what are the polynomials we will use for the ciphers, we will have to build them. We will have to check various properties using simulation and testing to be more confident whether our ciphers are indeed secure. Moreover, we will be able to compare various polynomials to see which one is the most efficient. We will also compare it with the original MiMC cipher to check if we perhaps improved it or at least got similar results. To do this, a program is written in Rust programming language. The choice for this language is the growing popularity due to its high performance\textcolor{red}{ideti cite}, reliability, great community and it is also author's subjective preference. As this project is more theory focused and not coding, the implementation part will discuss some key aspects of testing and rather than the overall software design.

Having implemented the ciphers and checked that the new ciphers are secure, we will check what are the possible applications of it. Since the original idea of MiMC was to design a cipher with the intent of being ZK-friendly, we will have to verify whether our new cipher is still applicable for this purpose\textcolor{red}{reread and check what are the actual results we got}.


% ------------- BUILDING NEW CIPHERS ----------------
\section{Building new ciphers}
\begin{itemize}
    \item What if we would try x\^ 3 - 1 or something?
    \item The math behind fields and how all operations work there.
    \item Why taking these polynomials. The idea is that we need a permutation polynomial over our selected field $\mathbb{F}_n$. Need to argue why it has to be a polynomial of this form. Then can show what are these permutation polynomials. Explain \(x^n is a permutation if and only if gcd(exponent, 2^n - 1) = 1\)\par
    \textcolor{blue}{Maybe use \textbf{GTDS} as well to get new polynomial and use that paper to show that our newly built cipher is still cryptographically secure?}
    \item \textcolor{blue}{Maybe take some polynomial which would not work $x^2$ and show why it doesn't work.}
    \item New cipher structure with new parameters (how many rounds, new field, block size etc)
    \begin{itemize}
        \item Could do normal MiMC and could do Feistel GMiMC maybe?
    \end{itemize}
\end{itemize}
\vspace{10pt}
In general, a block cipher can be described as a function of a pair:
\[
f : m \times k \to c,\quad f^{-1} : c \times k \to m,
\]
where $f$ encrypts the message $m$ with key $k$ and $f^{-1}$ decrypts the ciphertext $c$. Thus, we have our first definition:
\begin{definition}\label{def:1}
    A \textit{cipher} is a function $f$ which has an inverse $f^{-1}$ such that \(\forall k \in \mathcal{K}\) and \(\forall m \in \mathcal{M}\) we have:
    \[
        (f^{-1} \circ f)(m, k) = m.
    \]
\end{definition}
In practice, the domains of $\mathcal{M}$, $\mathcal{K}$ and $\mathcal{C}$ are finite, hence by Universal Mapping Theorem (Theorem 72\cite{CryptanalysisBook}), any cipher can be written as polynomial system of equations over $\operatorname{GF}(p)$, for any prime $p$, where $\operatorname{GF}$ is the Galois (finite) field. In general, working in Galois fields is extremely useful, as it allows to construct ciphers and also we will see that it allows for fast computational implementations\textcolor{red}{veliau parasyt, kad tikrai taip}. However, first, we need to understand the maths of how to work in Galois fields as our cipher will operate in $\operatorname{GF}(p^n)$. We have to properly define such simple things as addition and multiplication once again, as these operations behave differently than in the standard fields of $\mathbb{N}$, $\mathbb{R}$ or $\mathbb{C}$.

After that, we can try to look at MiMC and its' underlying APN $x^3$. Understanding why this particular polynomial works as a cipher is the key to building new ciphers in the same structure. Trying any polynomial and then hoping that we can find the inverse $f^{-1}$ which would decrypt our cipretext is the backwards way of thinking of how ciphers are actually created. We need to prove mathematically that for any message $m$ and any key $k$, we can indeed find the decryption function, such that \(f^{-1} \circ f = id\), where $id$ is the identity function. If we do that, then we have successfully created a cipher algorithm. However, this is just the first step of creating a properly cipher algorithm. What we are then tasked to do is show that it is secure.

\subsection{Field maths}\label{sub:field-maths}
As already observed, since the domains of our message $\mathcal{M}$, key $\mathcal{K}$ and ciphertext $\mathcal{C}$ are finite, we will be operating in a Galois (finite) field. However first, it is necessary to understand what a \textit{field} actually is. Thus, an introduction to groups, fields, and maths behind it is given in this subsection and first we start with groups, hence, we give a formal definition of a group:
\begin{definition}
    A group is a set of elements $G = \{a, b, c, ...\}$ and an operation $\oplus$ for which the following axioms hold:
    \begin{itemize}
        \item \textit{Closure}: for any $a \in G$, $b \in G$, the element $a \oplus b$ is in $G$.
        \item \textit{Associative law}: for any $a$, $b$, $c \in G$, $(a \oplus b) \oplus c = a \oplus (b \oplus c)$.
        \item \textit{Identity}: There is an identity element $0$ in $G$ for which $a \oplus 0 = 0 \oplus a = a$ for all $a \in G$.
        \item \textit{Inverse}: For each $a \in G$, there is an inverse $(-a)$ such that $a \oplus (-a) = 0$.
    \end{itemize}
\end{definition}
At first, it might be confusing what a group actually is but this is simple can be the integers under addition, denoted ($\mathbb{Z}, +$). Note that for any element $a \in \mathbb{Z}$, all the axioms are satisfied as addition does not leave the group, it does not matter in which way we add numbers, $0$ is in $\mathbb{Z}$ and we can always find an inverse $(-a)$. However, if we look at natural numbers under addition ($\mathbb{N}, +$), then this is not a group. This is because the \textit{identity} and \textit{inverse} axioms are not satisfied. For example $5 \in \mathbb{N}$ however, $(-5) \notin \mathbb{N}$, moreover, $0 \notin \mathbb{N}$. Thus, a group consists of two things: a set of elements and an operator which satisfy the four axioms. But we can add more axioms. In particular, if we add the \textit{commutative} axiom we get the \textit{abelian} group:
\begin{definition}
    An abelian group $G$ with operator $\oplus$ is a group which additionally satisfies the \textit{commutativity} axiom: for any $a \in G$, $b \in G$, $a \oplus b = b \oplus a$.
\end{definition}
This axioms is really useful when working algebraically, as it allows for easier calculations, implementations and will give us fields. Most of the well known groups are abelian ($\mathbb{Z}, +$), ($\mathbb{R}, +$), ($\mathbb{Q}, +$). However, there are groups which are not abelian, namely the non-invertable matrices under multiplication as $AB \ne BA$, for non-invertable matrices $A$ and $B$.

So far we have talked about groups with infinitely many elements, however we can have groups with finite amount of elements $G = \{a_1 , a_2, ..., a_n\}$, where $|G| = n$ is the order of $G$. Moreover, the operator $\oplus$ can then be specified by an $n \times n$ “addition table” whose entry at row $i$, column $j$ is $a_i \oplus a_j$ . The cancellation property implies that if $a_j \ne a_k$, then $a_i \oplus a_j \ne a_i \oplus a_k$ . Thus, all elements in any row $i$ of the addition table are distinct, i.e., each row contains each element of $G$ exactly once. Similarly, each column contains each element of $G$ exactly once. Thus the group axioms restrict the group operation $\oplus$ more than might be immediately evident.

One important example of finite abelian group is the the integers mod-$n$ group denoted $\mathbb{Z}_n$. This is a group where a set is the remainders $\{0, 1, ..., n - 1\}$ under mod-$n$ addition, where $n$ is a positive integer. This is actually a finite \textit{cyclic} group, i.e. it has a generator $g \in G$, such that each element of $G$ can be expressed as a the sum $g\oplus ... \oplus g$ for some number of repetitions.

Now we can talk about fields and we give a formal definition:
\begin{definition}
    A field is a set $\mathbb{F}$ of at least two elements, with two operations $\oplus$ (called addition) and $*$ (called multiplication), for which the following axioms are satisfied:
    \begin{itemize}
        \item The set $\mathbb{F}$ forms an abelian group with identity $0$ under the operation $\oplus$.
        \item The set $\mathbb{F}^* = \mathbb{F} - \{0\} = \{a \in \mathbb{F}, a \ne 0\}$ forms an abelian group with identity called $1$ under the operation $*$.
        \item \textit{Distributive law}: For all $a$, $b$, $c \in \mathbb{F}$, $(a \oplus b) * c = (a * c) \oplus (b * c)$.
    \end{itemize}
\end{definition}
Hence the field is almost the same as a group but with another operation and a law of how to distribute these two operations. Similarly like with groups, the usual examples for a field are ($\mathbb{R}$, $+$, $*$) and ($\mathbb{Q}$, $+$, $*$). However, ($\mathbb{Z}$, $+$, $*$) does not form a field since there is not multiplicative inverse. This can be fixed if we instead take the cyclic group $\mathbb{Z}_p$ for some prime $p$. As it turns out, this is indeed a field with operators $+$ and $*$, where multiplication is defined as usual integers multiplication but taking the remainder after division by $p$.
\begin{theorem}
    For every prime $p$, the set $\{0, 1, ... , p - 1\}$ forms a Galois (prime) field (denoted by $\operatorname{GF}(p)$ or $\mathbb{F}_p$) under mod-$p$ addition and multiplication.
\end{theorem}
The proof of this simply follows the definition of groups and fields\cite{FieldNotes}. Moreover, these fields are actually unique for the given $p$ and can be shown that any field $\mathbb{F}$ with a prime number of elements $p$ is isomorphic to $\mathbb{F}_p$ (by prime field uniqueness theoreom\cite{FieldNotes}). In fact, this is already enough for us to build a cipher and we can work in $\mathbb{F}_p$ field, however, in this project we are going to work with $\mathbb{F}_{p^n}$:
\begin{theorem}
    If $g(x)$ is a prime polynomial of degree $n$ over a prime field $\operatorname{GF}(p)$ (or $\mathbb{F}_p$), then the set of remainder polynomials with mod-$g(x)$ arithmetic forms a finite field denoted by $\operatorname{GF}(p^n)$ (or $\mathbb{F}_{p^n}$) with $p^n$ elements.
\end{theorem}
Proving this is done showing that the field axioms hold with modular polynomial arithmetic\cite{FieldNotes}, which need to be discussed.

\subsection{MiMC with general exponent}
Modular polynomial arithmetic is done the same way as with integers except we add and divide polynomials. This is because every polynomial $f(x)$ can be expressed as $f(x) = q(x)g(x) + r(x)$ for some polynomial remainder $r(x)$ and polynomial quotient $q(x)$. Then the coefficient operations are simply in $\mathbb{F}$. For example, \(x^3-2x^2-4 = (x^2+x+3)(x-3) + 5\), we can use Euclidean long division to verify this. Now similarly to primes in integers, there are also primes of polynomials.
\begin{definition}\label{def:prime-poly}
    A polynomial is \textit{prime} if it is \textit{monic} and is \textit{irreducible}. That is, the highest order coefficient is 1 and the polynomial has no trivial factors.
\end{definition}
Moreover, just like with integers, every monic polynomial can be uniquely expressed as a prime polynomial factorization. Thus, prime polynomials behave pretty much the same as the usual primes. The only difference being is that we are working in a different field. Particularly, for our cipher we are working in $\operatorname{GF}(2^n)$, that is, the message $\mathcal{M}$, key $\mathcal{K}$ and ciphertext $\mathcal{C}$ elements are actually elements of $\operatorname{GF}(2^n)$, for some $n \ge 2$. What this means, is that the elements have the form of $f(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_0$, where $a_0$, $a_1$, ..., $a_n \in \operatorname{GF}(2)$. Any field can be chosen for this, however, implementations of this field are way easier and this is discussed in chapter \ref{chapter:implementation}\textcolor{red}{duoti references, kodel butent GF(2)}. As discussed, our operations are modular polynomial operations, thus addition $+$ and multiplication $*$ is not like with integers.\medskip

------------MAIN START---------------

\noindent\textbf{MiMCGe} If we take a look at MiMC cipher\cite{MiMC}, it uses the same field $\operatorname{GF}(2^n)$, where $n$ describes the block size. The underlying polynomial for the cipher is $x^3$. However, we can in fact take any polynomial $x^e$, for any $e \ge 2$ and we call this the \textit{MiMCGe} cipher. We can prove that this does in fact give us a cipher (Fig. \ref{fig:cipher}) by our definition \ref{def:1}. The cipher algorithm is simple: plaintext $m$ is added to a random element (round constant) $r_i$ and key $k$. Then instead of cubic the result, it is raised to some exponent $e$ and this process is repeated for at least $R := \left\lceil n\log_e(2) \right\rceil$ number of rounds or for full security: \(R + \left\lceil \log_e(\frac{6R}{e}) \right\rceil\). The choice for this is discussed in subsection \ref{sub:3.1}. The round constants are chosen as random elements every time we initialise the cipher. However, these constants could also be predefined earlier similarly to AES.


\begin{figure}
    \centering
    \caption{MiMC with general exponent $e$ cipher algorithm\cite{MiMC}\cite{CipherGraphs}.}
    \label{fig:cipher}
    \begin{tikzpicture}
        \tikzstyle{every node}=[transform shape];
        \tikzstyle{every node}=[node distance=1.8cm];

        \node (XOR-1)[XOR,scale=1] {};
        %% (Start) message
        \node [left of=XOR-1] (p) {$m$};
            % Connecting message to first XOR
        	\path[line] (p) edge (XOR-1);
        %% Block
        \node (block-1) [right of=XOR-1,draw,rectangle,thick,minimum width=4em, minimum height=4em] {$x^e$};
            % Connecting block to XOR before
        	\path[line] (XOR-1) edge (block-1);
        %% XOR right of block before
        \node (XOR-2)[right of=block-1,XOR,scale=1] {};
        	\path[line] (block-1) edge (XOR-2);

        %% Second block
        \node (block-2) [right of=XOR-2,draw,rectangle,thick,minimum width=4em, minimum height=4em] {$x^e$};
        	\path[line] (XOR-2) edge (block-2);
        %% Third XOR
        \node (XOR-3)[right of=block-2,XOR,scale=1, node distance=2.4cm] {};
        	\path[dashed] (block-2) edge (XOR-3);

         %% Last block
        \node (block-3) [right of=XOR-3,draw,rectangle,thick,minimum width=4em, minimum height=4em] {$x^e$};
            \path[line] (XOR-3) edge (block-3);

        \node (XOR-4)[right of=block-3,XOR,scale=1] {};
            \path[line] (block-3) edge (XOR-4);

        %% Result
        \node (res)[right of=XOR-4] {$c$};
            \path[line] (XOR-4) edge (res);

        %% Subkeys
        \node (k-0) [above=0.6cm of XOR-1] {};
            \path[line] (k-0) node[above] {\small $k$} edge (XOR-1);
        \node (k-1) [above=0.6cm of XOR-2] {};
            \path[line] (k-1) node[above] {\small $k \oplus r_1$} edge (XOR-2);
        \node (k-2) [above=0.6cm of XOR-3] {};
            \path[line] (k-2) node[above] {\small $k \oplus r_{R-1}$} edge (XOR-3);
        \node (k-3) [above=0.6cm of XOR-4] {};
            \path[line] (k-3) node[above] {\small $k$} edge (XOR-4);
    \end{tikzpicture}
\end{figure}


First, we need to prove that indeed this is a cipher hence let us see why indeed we can take any power $e$. For this, we need to understand what a permutation polynomial is:
\begin{definition}
    A polynomial $f \in \operatorname{GF}(q)$ is called a \textit{permutation polynomial} of $\operatorname{GF}(q)$ if the associated polynomial function $f : c \to f(c)$ is a permutation of $\operatorname{GF}(q)$.
\end{definition}
Moreover, remember, that in order to have a cipher, we need to have a function $f$ with an inverse $f^{-1}$, such that $f^{-1} \circ f = id$. However, if our encryption is essentially a permutation polynomial of the field, then we are guaranteed that there exists an inverse (decryption) function. Luckily, we can use a theorem for this:
\begin{theorem}
    A polynomial $x^e$ is a permutation polynomial of the field $\operatorname{GF}(q)$ if and only if gcd($e$, $q - 1$) $= 1$.
\end{theorem}
The proof of this uses Hermite's criterion\cite{PPIntro}\textcolor{red}{better to prove myself}. Thus, to have a cipher it is needed to get an inverse function. For that, it is necessary that the degree $e$ and $q - 1 = 2^n - 1$ are coprime, where $n$ is the block size. Now to build an inverse (decryption) function given $a^e = b$, integer $s$ has to be found such that $b^s = a$, where $a$ and $b$ are in $\operatorname{GF}(2^n)$. However, first we an additional lemma:
\begin{lemma}\label{lem:GroupProp}
    The property $a^{q-1} = 1$ holds for any element $a \in \operatorname{GF}(q)$.
\end{lemma}
\begin{proof}
    % Nezinau cia su situo, niekur nerandu, todel arba bandyt palikt, arba paklaust lectures, arba det reference koki i pvz sita: https://www.math.columbia.edu/~khovanov/ma2_fall/files/finitefields.pdf
    \textcolor{red}{labai nepatinka sitas is wiki, bet nerandu geresnio niekur} Let the field be $\operatorname{GF}(q)$. Note that the equation $x^k = 1$ has at most $k$ solutions in any field hence, $k \le |\operatorname{GF}(q)| = q - 1$. Moreover, by Lagrange's theorem\textcolor{red}{gib}, there exists a divisor $k$ such that $x^k = 1$ for every non-zero $x$ in $\operatorname{GF}(q)$. Putting these two together we get that $q-1$ is the lowest value to $k$, i.e., $k = q - 1$.
\end{proof}
Notice, that when $q$ is prime, this is equivalent to the Fermat's little theorem. Now we can use this to prove our result:
\begin{theorem}
    The inverse element of $b = a^e$ in $\operatorname{GF}(2^n)$ is given by $b^s$, where $s := \frac{t(2^n-1) + 1}{e}$, for some $t$. If gcd($e$, $2^n-1$) $= 1$, then $s$ and $0 < t < e$ are integers.
\end{theorem}
\begin{proof}
    It consists of two parts. First we prove that in fact $s$ exists, then that $s$ and $t$ are integers. We begin with the fact that we need to find $s$ such that in $\operatorname{GF}(2^n)$, we have:
    \[
        b^s = (a^e)^s = a^{es} = a.
    \]
    Using lemma \ref{lem:GroupProp}, we have that $a^{2^n-1} = 1$ in $\operatorname{GF}(2^n)$. Moreover, we can raise this to some power $t$ and then multiply by $a$ to get:
    \begin{align*}
        a^{2n-1} &= 1\\
        (a^{2n-1}) ^ t &= 1^t = 1\\
        a (a^{2n-1})^t &= a\\
        a^{t(2n-1)+1} &= a
    \end{align*}
    Note, that $1^t=1$ by the second axiom of the field, which says that $1$ is the multiplicative identity. Thus we get that \(es = t(2^n-1)+1 \Rightarrow s := \frac{t(2^n-1)+1}{e}\).\medskip

    \noindent Now to prove that $s$ is an integer we need that:
    \begin{equation}
        \begin{array}{c}
            s = \frac{t(2^n-1) + 1}{e} \iff t(2^n-1) + 1 \equiv 0 \mod e
        \end{array}\label{eq:1}
    \end{equation}

    \noindent And this can be simplified to \(t(2^n-1) \equiv -1 \mod e\). For this, we can use the modular multiplicative inverse property which states that $ab \equiv 1 \mod m$ if and only if gcd($a$, $m$) $= 1$. And this is exactly the same case if we multiply by $-1$, since gcd($e$, $2^n-1$) $= 1$. Thus, we get that we can indeed find an integer $t$, such that the equation \eqref{eq:1} is satisfied, giving us an integer $s$. Moreover, if $t \ge e$, then $t$ can be replaced by $t \mod e$ and also, since $\frac{1}{e}$ is not an integer, $t \ne 0$. Giving us that $0 < t < e$.
\end{proof}
This means that since the encryption is a permutation polynomial if and only if gcd($e$, $2^n-1$) $= 1$, then the decryption exponent will also be an integer. Thus, the decryption function is of the same form but with a different power $s := \frac{t(2^n-1)+1}{e}$. Notice, that $e \ll s$, hence decryption is considerably slower and methods to compute fast exponentiation are necessary\textcolor{red}{ideti ref, kai bus parasyta}.
\textcolor{red}{Realiai kazkaip biski silpnokai atrodo, nes dauguma theorems neirodyta, o tiesiog pasakyta, tai gal kokia viena irodyt butu gerai.}

% ------------- SECURITY ANALYSIS ----------------
\section{Security analysis}
\begin{itemize}
    \item Prove them together, that they still have cryptographic properties
    \item Need to argue how the number of rounds have been chosen by looking at the original paper and explaining why it's chosen that way. Then using that, our own ciphers have different round number.
    \item Need to talk about paper where they broke MiMC.
    \item \textcolor{red}{Create a graph which shows depending on rounds, what the confusion/diffusion rates are!! Also why showing for all is impossible (too many to test).}
    \item In the end talk about what are some other techniques people use to analyse security but are too advanced for this paper (linear/differential cryptanalysis).
    \item Show using graphs, NIST STS that these still hold and explain why we can't just say it works, i.e. it's an open problem on how to actually measure nonlinearity \begin{itemize}
        \item Diffusion
        \item Confusion
        \item Avalanche effect
        \item key sensitivy
        \item key space
        \item \textcolor{blue}{Maybe linear/differential cryptanalysis. Would be really nice to do this}
        \item \textcolor{blue}{Maybe something else not basic? Like some other specific attacks}
    \end{itemize}
\end{itemize}

% https://crypto.stackexchange.com/questions/10478/why-is-aes-considered-to-be-secure
% Teoriskai negalima sukuri unbreakable. Good rabbit hole, jei reik daugiau background: https://crypto.stackexchange.com/questions/87316/is-it-theoretically-possible-to-create-an-unbreakable-cipher.

% Labai geras video https://youtu.be/GQX8W8zKf2Q?t=1026 apie cipher breaking, kad upper bound yra bruteforce. Gal ideti, kaip intro dar papasakoti


What we are now tasked to do is show that the cipher is indeed secure. However, proving security is the hardest step which requires deep knowledge of cryptanalysis, hence, this project is not an extensive dive into the realms of cryptography analysis. Rather, this is just an introduction of what are some techniques that are used to argue and prove that the cipher is secure.

Even the formerly most used ciphers get broken and replaced by other standards (for example DES was broken with introduction of differential cryptanalysis\cite{DESBreak} and later replaced by AES). This is because it is possible to prove that a cipher is secure against some attacks but it is impossible to create a cipher which cannot be broken at all. For that, we need information-theoretic security introduced by Claude Shannon who proved that one-time pad is not breakable with infinite computational power\cite{OneTime}. However, key needs to be at least as big as the plaintext and can be used only once. Hence, what is more commonly used is the computational security which depends on the fact that some problem is considered "hard". For example RSA relies on the fact that large number factorisation is a computationally hard problem. Thus, one of the ways we argue that the cipher is secure is showing that constructing some attack is as computationally hard as brute forcing.

\subsection{Round number}\label{sub:3.1}
Choosing the round number is crucial as we want it to be smaller for efficiency but this comes at the cost of security. Thus, the goal is always to choose it as small as possible to thwart any possible attacks. A few of these attacks are statistical in nature where they try to predict some behaviour from randomness. These are linear\cite{DESLinear} cryptanalysis which try to approximate cipher as a linear function and differential cryptanalysis\cite{DESBreak} which looks at differences through cipher which lead to higher probability in ciphertext allowing to recover parts of the key. However, MiMC was designed based on Knudsen-Nyberg cipher\cite{KNCipher} which was designed to mitigate differential cryptanalysis. But there are also algebraic attacks such as interpolation\cite{InterpolationAttack} or higher-order differential attacks\cite{HigherOrderAttack1}\cite{HigherOrderAttack2} which are especially strong against a cryptographic scheme with simple algebraic representation. In fact, Knudsen-Nyberg cipher was broken with higher-order differential attack\cite{InterpolationAttack} and MiMC has also been broken with the same attack\cite{MiMCAttack}. In the paper, it is conjectured that the number of rounds necessary to prevent higher-order differential attacks is also sufficient to prevent interpolation attacks. Thus, the motivation for choosing the round number is based on these attacks.\medskip

\noindent\textbf{Higher-Order Differential Attack.} The main way this attack works is similar to differential cryptanalysis, however, instead of taking the first derivative, higher-order derivatives are taken. These derivatives are defined as functions $f$ with respect to some $\alpha \in \mathbb{F}^n_2$:
\[
    \frac{\delta}{\delta \alpha}f(x) = f(x) + f(x + \alpha).
\]
Then we can take k-order derivative with elements $\alpha_1, ..., \alpha_k \in \mathcal{V}$ in vector space $\mathcal{V}$ as follows:
\[
    \frac{\delta}{\delta \alpha_1}\;...\;\frac{\delta}{\delta \alpha_k} f(x) = \bigoplus_{w \in V + x}f(w).
\]
These have useful derivative properties such as sum rule, product rule and more importantly, taking the derivative reduces the degree. Then, a distinguisher is taken, for example the zero sum of size $s$ is a set $\{x_1, ..., x_s\} \subseteq \mathbb{F}^n_2$ such that:
\[
    \sum^s_{i=1}x_i = \sum^s_{i=1}f(x_i) = 0.
\]
If $f$ is truly random, then this should be hard to find, if not, we can distinguish from randomness. What higher-order differential attack does is exploit the low algebraic degree (definition \ref{def:3.1}) ciphers and build key-recovery attacks based on distinguishers\cite{CryptanalysisCource}. Thus, to prevent this attack, the cipher must reach the maximum algebraic degree well within the number of rounds.
\begin{definition}\label{def:3.1}
    The \textit{algebraic degree} is the highest degree of any term in the algebraic expression which describes the output bits.
\end{definition}
It is difficult to know exactly what this degree is hence, estimations are used. The growth of algebraic degree heavily depends on the cipher structure and number of rounds. In the original design, it was incorrectly assumed that it grows exponentially in the number of rounds, however, in the MiMC attack\cite{MiMCAttack} it was discovered that it was rather almost linear. In fact, they got that if the number of rounds \(r < \left\lceil \log_e(2^{n-1}-1) \right\rceil\), where $e$ is the degree of a round function, then a higher-order distinguisher using at most $2^{n-1}$ data can be used (Proposition 2\cite{MiMCAttack}). Note, that $e$ always refers to the \textit{exponent} and not the Euler's number. If we set block size to be equal to 129, then this upper bound is \(\left\lceil \log_3(2^{128}-1) \right\rceil = 81\), while the actual number of rounds is \(\left\lceil 129\log_3(2) \right\rceil = 82\), leaving only 1 round to break. This is done by building and solving univariate polynomial $F(K)$ using known-ciphertexts. This requires $2^{n-1}$ ciphertexts making the attack not practical and it is not intended to replace the original round number estimation as discussed by the authors. However, this gives a great insight of the weakness of the cipher and a new algebraic degree growth bound.

In essence, it means that about \(\left\lceil n\log_e(2) \right\rceil - 1\) rounds can be set up using zero sum. Thus, the rest of the rounds need to be sufficient enough to make construction of univariate polynomial $F(K)$ infeasible. We follow similar way as described in the paper (Section 5.4\cite{MiMCAttack}) but with our general exponent $e$. We use the fact, that the complexity to construct the polynomial $F(K)$ is equal to \(2^{n-1}(\frac{e^{\rho + 1}-1}{2}) = 2^{n-2}(e^{\rho + 1}-1)\), where $\rho$ is the new round number, we want this to be greater than simple exhaustive search \(2^n(R+\rho)\), where \(R = \left\lceil n\log_e(2) \right\rceil \) is the original round number we get:
\begin{align*}
    2^{n-2}(e^{\rho + 1}-1) &\ge 2^n(R +\rho)\\
    e^{\rho + 1} &\ge 4(R +\rho) + 1\\
    \rho &\ge \log_e(4(R+\rho) + 1) - \log_e(e)\\
    \rho &\ge \log_e\left(\frac{4(R+\rho) + 1}{e}\right) \ge \log_e\left(\frac{6R}{e}\right)
\end{align*}
Where we suppose that \(\rho \le R/2\). Thus, we get that to thwart higher-order based key-recover attack, it is necessary to take \(\rho \approx \log_e(\frac{6R}{e})\) additional rounds. This number is relatively small, for example with block size 129 and exponent 5, $\rho \approx 3$, while with exponent $24$ it is only $\rho \approx 1$. Thus, the lower round MiMCGe has \(R := \left\lceil n\log_e(2) \right\rceil\) rounds, where $e$ is the exponent and $n$ is the block size, while the full round MiMCGe cipher has \(R + \left\lceil \log_e(\frac{6R}{e}) \right\rceil\) rounds.

\subsection{Diffusion and confusion}
\textcolor{red}{Should really look into \href{https://eprint.iacr.org/2010/564.pdf}{this}}
In this subsection instead of mathematically proving cipher security we argue it based on statistical data obtained from testing. The core principle this is based on is the Claude Shannon's principle which states that each input and key bit must \textit{influence each} output bit in a \textit{complicated} way\cite{OneTime}. The two properties described here are \textit{diffusion} and \textit{confusion} which can be defined as:
\begin{definition}
    \textit{Diffusion} means that a small change in the plaintext results in a significant change in the ciphertext.
\end{definition}
\begin{definition}
    \textit{Confusion} means that each bit of the ciphertext should depend on several parts of the key, obscuring the connections between the key and the ciphertext.
\end{definition}
Diffusion can also be associated with a strict avalanche criterion which states that changing one bit of the plaintext should change each bit of the ciphertext with a probability of one half (a coin flip). Confusion is a similar concept but changing one bit of the key instead of the plaintext should similarly change about half the bits of the ciphertext. In essence, all secure block ciphers should have these properties. For stream ciphers, diffusion is harder to achieve as they do not work with the whole block. However, since we are working with a block cipher, it is possible to test these properties to see if they hold.\medskip

\noindent\textbf{Performing the tests.} Two tests were peformed: diffusion \ref{alg:diff} and confusion \ref{alg:conf}, where the only difference is flipping either plaintext or key bits. The goal is to check how many bits of the new ciphertext change, when flipping only a single bit. In the perfect scenario, the goal is to have \(r / n^2 = 0.5\), where $n$ is the block size. That is, flipping a single bit is a perfect coin flip of the output. The denominator is squared since the tests iterate over the whole plaintext and flipping the bit $n$ times. In reality, it is not possible to check very single plaintext and key pair. This is because there are $2^n$ unique choices for the plaintext and $2^n$ unique choices for the key as they are elements of $\operatorname{GF}(2^n)$. It means that there are \(2^n*2^n = 2^{2n}\) different combinations in total assuming round constants are always the same. Hence, only a sample is being tested and the goal is to have \textbf{diffusion and confusion rate of $ 50\% \pm 0.05\%$}.

\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \caption{Diffusion test}\label{alg:diff}
    \begin{algorithmic}
        \Require random plaintext $p$ with key $k$.
        \State $r \gets 0$
        \State $c \gets encrypt(p, k)$
        \For{$i$ bit in $p$}
            \State $p_i \gets p[i] \oplus p[i]$ \Comment{Flip $i^{th}$ bit}
            \State $c_i \gets encrypt(p_i, k)$
            \State $r \gets r + \operatorname{count\_flipped\_bits}(c, c_i)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \caption{Confusion test}\label{alg:conf}
    \begin{algorithmic}
        \Require random plaintext $p$ with key $k$.
        \State $r \gets 0$
        \State $c \gets encrypt(p, k)$
        \For{$i$ bit in $k$}
            \State $k_i \gets k[i] \oplus k[i]$ \Comment{Flip $i^{th}$ bit}
            \State $c_i \gets encrypt(p, k_i)$
            \State $r \gets r + \operatorname{count\_flipped\_bits}(c, c_i)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}

The tests were run 10000 times with four different exponents ($x^3$, $x^5$, $x^{16}$, $x^{24}$) over the block size 31 for reduced round MiMCGe cipher and the results are displayed in the graphs \ref{fig:diffconftests}. The exponents were chosen with the intent to compare the original $x^3$ with others which performed the best as discussed more broadly \textcolor{red}{prideti (sub)section}. Additionally, round reductions were applied to see how diffusion and confusion depends on the round number. For example round reduction of $0$ means $\left\lceil 31\log_e(2) \right\rceil$ rounds, while $10$ for $e = 3$ means \(\left\lceil 31\log_3(2) \right\rceil - 10 = 10\) rounds.

% \begin{figure}
%     \hspace{-1cm}
%     \centering
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/DiffusionConfusionx3}
%     \end{minipage}\hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/DiffusionConfusionx5}
%     \end{minipage}

%     \hspace{-1cm}
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/DiffusionConfusionx16}
%     \end{minipage}\hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/DiffusionConfusionx24}
%     \end{minipage}
%     \caption{Diffusion and confusion test results of chosen ciphers}\label{fig:diffconftests}
% \end{figure}

From the tests, it is clear that indeed the ciphers have sufficient diffusion and confusion. When no round reduction is applied, all ciphers, except $x^{16}$ which is discussed separately, have diffusion and confusion rates of $50\% \pm 0.01\%$ which achieve the predefined goal. Moreover, these diffusion and confusion rates stay consistent as the number of rounds decreases. They do not exceed $50\% \pm 0.05\%$ for almost the whole time. This threshold is exceeded only when the ciphers operate with two or less rounds. Note, that when performing the tests, it was observed that increasing the test size (number of times the test is repeated), gives rates which are closer to $50\%$. For example\textcolor{red}{sita isanaliztuoti dar, nes nebutinai berods. Sakyti max 10k same as 1M ir paaiskinti arba cia su table, arba appendix, bet is dalies kazkaip atrodo mazai conclusion del testu.}\medskip

\noindent\textbf{Powers of two case.} The interesting case is when the exponent is a $x^{16}$. It is evident, that this is not because the exponent is even, as $x^{24}$ achieves the specified goal of diffusion and confusion. This only happens with exponents in a form of a power of two and it is clear that these ciphers fail diffusion and confusion as they do not even reach $40\%$, hence we state that \textbf{these are not secure ciphers} as they do not show sufficient non linearity. Analysing further, it was observed that diffusion and confusion rates stays the same no matter what test size is given. That is, testing one or $1000$ different plaintexts yields the same diffusion and confusion percentages even though each time random plaintexts and key is generated.\textcolor{red}{Reikia toliau analizuoti, duoti ne tik $x^{16}$. Labai svarbu gauti conclusion, kodel taip yra!! (gal subfields $2^n$?)}

\subsection{Pseudo-random number generator}\label{chapter:prng}
Another way the ciphers can be looked as is through random number generators. If a cipher is truly secure, then it must be non-linear and the mapping of the input is random. That is, if we give an input, it should map to the output in a random and unpredictable way. Hence, a cipher is also a pseudo-random number generator. This is, however, a consequence of producing a cipher rather than the actual objective. Thus, when arguing security, formal mathematical analysis is always more prioritised over statistical analysis. However, it is a great tool which can easily demonstrate if the cipher is insecure.

Note, that it is impossible to generate truly random numbers in any mathematical way. In fact, even in physics, the sources of true randomness are rare. Hence, the term \textit{pseudo}-random number generators (PRNG) is used when speaking about random number generators, which are software based. The study of randomness is a an active field of research with recent papers using quantum physics for zero-knowledge proofs\cite{QuantumZK} or developing generators for FHE\cite{LatticeRNG}. Thus, there are tools created specifically for testing these generators, notably NIST STS\cite{NIST} and Dieharder\cite{Dieharder} which are considered to be industry standard for testing cryptographic algorithms\cite{AESNI}.

The underlying theory which all of these testers utilise is the \textit{Central Limit Theorem} (CLT). The idea is that the PRNG generators produce a stream of some numbers on a specific range (floating point numbers, 32 bit, etc.). Then to construct an experiment, sum up all the numbers to get $t$ which should give a mean value of $\mu = t/2$. Now taking a large amount of these experiments produces independent and identically distributed sums $t_i$. By the CLT, if PRNG indeed produces uniformly distributed numbers, the standard deviation is $\sigma = \sqrt{t/12}$. That gives the \textit{null hypothesis} such that:
\[
    t_1, ..., t_n \overset{iid}{\sim} N(\mu, \sigma^2)
\]
Now a $p$-value test is constructed which gives the probability of obtaining the sum $x$ from some experiment $t_i$. This is given by:
\[
    p = \operatorname{erfc}\left(\frac{|\mu - x|}{\sigma\sqrt{2}}\right)
\]
If this value is really low or really close to 1 with predetermined significance $\alpha$, then it means that the PRNG is biased towards some numbers particular numbers, thus, the null hypothesis is rejected. However, passing the test does not mean that the PRNG is indeed random. With null hypothesis testing, only rejecting is possible. If the null hypothesis is not rejected, it only gives higher certainty, that it is indeed true. Thus, numerous tests are performed to increase this confidence.\medskip

\begin{table}[]
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            Function & Pass & Fail & Result & Rate \\
            \hline
            $x^3$ & 15 & 0 & 187/187 & 100\% \\
            $x^5$ & 15 & 0 & 186/187 & 99.47\% \\
            $x^{16}$ & 1 & 12 & 1/162 & 0.62\% \\
            $x^{24}$ & 15 & 0 & 187/187 & 100\% \\
            \hline
        \end{tabular}
        \caption{NIST test results.}\label{tab:NIST-results}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            Function & Pass & Weak & Fail (Total) & Rate$_{norm}$ \\
            \hline
            $x^3$ & 69 & 6 & (39) & 0\%\\
            $x^5$ & 73 & 13 & (28) & 0\% \\
            $x^{16}$ & 0 & 0 & (114) & 0\% \\
            $x^{24}$ & 63 & 12 & (39) & 0\% \\
            \hline
        \end{tabular}
        \caption{Dieharder test results.}\label{tab:dieharder-results}
    \end{minipage}
\end{table}

\noindent\textbf{NIST STS.} It is a collection of 15 tests developed by the National Institute of Standards and Technology (NIST) aimed at assessing the randomness or statistical properties of PRNG or cryptographic algorithms. The tests require an input file at least of 10000 random stream of bits. For accurate results, there has to be at least 55 bit streams (as stated in NIST manula Section 4.2.2\cite{NIST}). In MiMCGe case, four different exponents ($x^3$, $x^5$, $x^{16}$, $x^{24}$) were tested with a block size of 127. 1000 bit streams were generated of 7875 different numbers which yield to $127 * 7875 = 1000125$ size per bit stream. In total, the input size is $\sim$1GB. The generated file simply consists of encrypted sequential numbers, where the random seed corresponds to the key together with round constants. This seed was chosen randomly and is the same for all the different exponents and tests. The implementation of generating this file is discussed in chapter\textcolor{red}{ref}.

At first, two control tests were performed. First is the built in test which checks a known PRNG against the test suite to check whether the tests correctly identify PRNG. The second control test was to see whether the construction of the input file is generated correctly. For this, the numbers were not encrypted and a file of just sequential numbers was given to the the test suite. This correctly identified that 15 out of 15 tests fail. Now, the files with encrypted numbers were given and the results are provided in the table \ref{tab:NIST-results}. The result column indicates how many of the test experiments were passed, out of all performed. Note, that some tests have multiple experiments (i.e. \textit{Non-overlapping Template Matching Test} has 147 experiments). The only experiment $x^5$ failed was the non-overlapping template experiment, however even then, the result was 979/1000, while the pass rate is 980/1000\textcolor{red}{appendix full result}. It is evident, that indeed, as previously discussed, powers of two exponents are not secure ciphers. The test suite could not even perform the \textit{Random Excursions} and \textit{Random Excursions Variant} tests for $x^{16}$. However, NIST STS test suite gives more confidence that the other ciphers are indeed secure.\textcolor{red}{pasneket apie ta testa, kuri x16 praejo}\medskip

\noindent\textbf{Dieharder.} It is another battery of statistical tests for pseudo-random number generators. Dieharder was developed as an extension of the original Diehard battery of tests by George Marsaglia\cite{Diehard}. This test suite is similar to NIST STS, however it has 31 different tests and the biggest difference is the input size. This test suite requires considerably larger sample space which can be anywhere close to $\sim$240GB. If the input file is smaller, then the tests rewind the file and go through it again. This can cause inaccurate results, however, the test suite also works with a random bit stream. Hence, for this test, instead of a file, a stream of encrypted sequential 32 bit numbers was provided. Implementation details are discussed in chapter\textcolor{red}{ref}.

Once again, the same two control tests were performed to check the test suite and the implementation of the input. Then the same exponents ($x^3$, $x^5$, $x^{16}$, $x^{24}$) were tested and the results are given in the table \ref{tab:dieharder-results}\textcolor{red}{argumentuoti weak ner blogai}. Note that, since these tests require considerably larger input, it took $\sim$\textcolor{red}{KIEK} days to get the full results, while NIST STS took around 8 hours.\textcolor{orange}{Galbut galima sakyt su 1GB buvo per mazai ir ka gavom} However, as before, it is evident, that once again we are proven that $x^{16}$ is not a secure cipher, while other exponents perform as expected. \textcolor{red}{daugiau paanalizuoti, kai bus iki galo pabaigti testai}

In general, these two test suites give a way to verify that a cipher does behave like a pseudo-random number generator. However, it is important to note, that they do not prove security. The underlying test which they perform is the null hypothesis testing which can only reject the hypothesis. It is, therefore, impossible using these tools to prove the null hypothesis. But even so, when these tools fail to reject the null hypothesis, they in turn give more confidence, that the ciphers are in fact secure.  

\textcolor{red}{Butu gerai dar kokia ataka ar kas. Galbut tokie mazi side notes apie ivairais atakas ir kaip jos veikia. Paklausti supervisor ar uztenka}

---------------MAIN END---------------

% ------------- IMPLEMENTATION ----------------
\section{Implementation}\label{chapter:implementation}
\begin{itemize}
    \item Design subsection which says that for example elements are Vec<u8>. Could also say first struggles, kad ilgai, kol isejo padaryt decryption.
    \item Testing subsection which says how we test field math, how ciphers are tested, evaluation criteria.
    \item Write code and perform benchmarks
    \item \textcolor{blue}{Analyse found results. Are they faster/better/worse. Conclusions} Sitas gal geriau in conclusion chapter.
    \item Future work which explains that we could further optimize it using precalculated tables, etc.
\end{itemize}

Having done all the theoretical work, it is necessary to move to the practical cipher implementation. This comes with its own challenges when tackling software design and optimisation problems. In this project, a CLI tool was built which implements MiMCGe cipher\footnote{\label{note:git}The code is accessible on \href{https://git.cs.bham.ac.uk/projects-2023-24/sxt063}{Gitlab}.}. It allows to test encryption and decryption, run diffusion and confusion tests, generate test inputs for the NIST STS and dieharder test suites. For comparison, AES and original MiMC ciphers are also implemented which can be used instead of MiMCGe.

\subsection{Design}
The git project\footref{note:git} has five directories:
\begin{itemize}
    \item \textit{/scripts}. Contains the utility shell scripts which were run to get test results for diffusion and confusion and to generate test data for statistical tests.
    \item \textit{/src}. Contains the implementation of the CLI tool of the cipher written in Rust.
    \item \textit{/test-suite-inputs}. Contains the generated input files used for the statistical test suites.
    \item \textit{/test-suite-results}. Contains the results generated from the statistical test suites.
    \item \textit{/tests}. Contain integration tests for the CLI tool.
\end{itemize}
The CLI tool is written in Rust, thus it first has to be installed on the system, then the user can compile the code. There is a supplementary \textit{README.md} file which has user manual together with some useful examples. Moreover, the code is well documented and the tool itself provides help when run with the \verb|--help| flag. For example, encrypting 2026 with key 432, exponent $x^7$ in a block size of 11 can be run as:
\begin{verbatim}
$ ./mimcge cipher-test mimcge 11 -p 2026 -e 7 -k 432
Plaintext:  2026 [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0]
Ciphertext: 1067 [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1]
Decrypted:  2026 [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0]
Time: 20.67µs
\end{verbatim}
The program prints the plaintext in decimal form and as a bit vector, then the corresponding ciphertext and its' bit vector together with decrypted ciphertext and the time it took to encrypt and decrypt.\medskip

\noindent\textbf{Implementing field arithmetic.} As discussed in chapter \ref{sub:field-maths}, basic operations are not the same as with standard fields $\mathbb{N}$ or $\mathbb{R}$. Thus, field arithmetic needs to be implementing from scratch. We start by creating a type for the element of the field: \verb|FieldElement| which will essentially be a vector of 1s and 0s (bits). This is because the elements of the Galois field $\operatorname{GF}(2^n)$ are polynomials, thus, the elements in the bit array represent the coefficients of the polynomial. That is, if we have an element $x^5+x^3+x^2+1 \in \operatorname{GF}(2^3)$, then the corresponding \verb|FieldElement| will be of length $2^3 = 8$ and look as: \verb|[0, 0, 0, 1, 0, 1, 1, 0, 1]|. This shows one of the benefits of implementation when working with the finite in the form of $\operatorname{GF}(2^n)$, as elements are just simple bit arrays. 

Now addition, multiplication and exponentiation need to be implemented as these are the only operations the cipher requires. First, notice, that adding two polynomials is essentially a XOR operation of the coefficients. For example, adding $x^5 + x^3 + x^2 + 1$ with $x^4 + x^2$:
\begin{table}[h]
    \centering
    \begin{tabular}{l r}
        & $1x^5 + 0x^4 + 1x^3 + 1x^2 + 0x + 1$ \\
        +& $0x^5 + 1x^4 + 0x^3 + 1x^2 + 0x + 0$ \\
        \hline
        & $1x^5 + 1x^4 + 1x^3 + 0x^2 + 0x + 1$
    \end{tabular}
\end{table}

Thus, implementing this is rather simple and this is yet another benefit of using the powers of two for the finite fields.

However, with multiplication, problems arise. This is because when two polynomials are multiplied, the total degree increases, however to not break the closure axiom, elements must stay in the field. To tackle this problem, the resulting polynomial needs to be reduced by an irreducible polynomial (definition \ref{def:prime-poly}). This is similar to modular arithmetic, where $a * b$ is defined as $(a * b) \mod p$, for some prime $p$. There are a few ways to implement this, including the naive implementation following the definition, however, the more efficient one is the one using the peasant algorithm \ref{alg:mult}. This algorithm utilises doubling and halving numbers while discarding the remainder, which is very efficient since it is essentially only shifting bits left or right. To utilise this algorithm for finite field arithmetic, the only two changes needed is changing the addition, which, as discussed previously, is the XOR operation. Another change needed is to check if $a$ escapes the field when doubling. That is, if working in $\operatorname{GF}(2^3)$ and $a$ already has the term $x^7$, then after doubling, it must be reduced by an irreducible polynomial. Note that, doubling $a$ is the same as with integers: shifting bit to the left. It also means, that for every different finite field, irreducible polynomial must be given. There are ways to find such polynomial, however, they were supplied beforehand. This means that only selected $\operatorname{GF}(2^n)$ are implemented and since $n$ means the block size, only some number of block sizes are valid as multiplication is not defined for others.

The only operation left is exponentiation. This can also be implemented naively using the definition, that $a^e = \overbrace{a * ... * a}^e$ since multiplication is already defined. However, notice, that decryption exponent is considerably larger (i.e. for block size $n = 33$ and exponent $e = 5$, decryption exponent $s := \frac{t(2^n-1)+1}{e} = \frac{4(2^{33}-1)+1}{5} = 6871947673$). It was tested that with naive implementation, decryption of block size 47 took $\sim$30 min. Thus, some method to minimise the number of multiplications is needed. There are a few ways to do this. One of the more popular methods is using pre-computed tables. However, they require a lot of data and also since this tool allows variable block size, square and multiply algorithm \ref{alg:exp} was chosen. This algorithm similarly to multiplication, utilises halving and doubling but in this case instead it is squaring. This algorithm brings down multiplications to around $\mathcal{O}(\log(e))$, where $e$ is the exponent. This allows to bring back decryption time into practical use and the same decryption of block size 47 now takes $\sim$1$\mu s$\textcolor{red}{patikrinti abu}.

% https://www.cs.uaf.edu/2015/spring/cs463/lecture/02_09_mul_exp.html
\textcolor{red}{parasyt normaliai apie improvement, kad exponential improvement. Dar kad tiesiog changing addition to multiplication means from multiplication to exponentiation}


\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \caption{Peasant multiplication}\label{alg:mult}
    \begin{algorithmic}
        \Require $a$, $b$.
        \State $result \gets 0$
        \While{$a > 0$ and $b > 0$}
            \If{$b$ is odd}
                \State $result \gets result + a$
            \EndIf
            \State Double $a$
            \State Divide $b$ by half discarding the remainder
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \caption{Square and multiply}\label{alg:exp}
    \begin{algorithmic}
        \Require $a$, $exponent$.
        \State $result \gets 1$
        \While{$exponent > 0$}
            \If{$exponent$ is odd}
                \State $result \gets result * a$
            \EndIf
            \State Double $a$
            \State Divide $exponent$ by half discarding the remainder
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\end{minipage}

One important thing to note is that these algorithm do not operate in constant time. This can lead to side channel attacks which exploit not the cryptographic algorithm itself but the implementation of it. Because of the if statement in the loop, this implementation could be exploited with a timing attack. That is, the attacker by measuring time taken for certain operations could infer information about the key or the input data. To mitigate these issues there are different variations of these algorithms which are not discussed here as it deviates from the goal\textcolor{red}{in conculsions parasyt, kad galima istirti, nes realiai tikslas ne implementation}.

\textcolor{red}{Siaip dar gal biski pakalbet pries field arithemtic apie kazka}

\subsection{Testing}
An important software development aspect is testing. The two main types of software testing are \textit{unit} and \textit{integration} testing. The former tests just a tiny portions of the code, for example if converting a binary array to the decimal expression is correctly implemented, while the latter tests from the users perspective, i.e. running the tool produces the expected output. Every good software should have both type of tests. How much testing is done, depends on the programmer as too many can slow down development, while not enough run the risk of having software with bugs. Thus, only the key parts of the software are tested.

Integration test file, following the conventional Rust file structure, is located in the \textit{/tests} directory. It contains simple tests which check that then the tool is run with specific block size, key, plaintext and round constants, the expected value is printed to the standard output. This is the \textit{happy path} testing, where with the predefined inputs, the program successfully finishes, providing the expected output. Additionally, tests are run which intentionally expect the program to fail. In that case, an error message is checked whether it has an expected output. This way the CLI tool is checked whether the correct guide on how to properly run the program is provided for the user.

The more interesting tests are the unit tests. These are located in the \textit{/src/tests} directory. There is a larger amount of these tests than integration tests. This is because every aspect of field arithmetic is checked. These tests check that the five main axioms (associativity, identity, inverse, commutativity, distributivity) hold for every implemented block size. These axioms are checked for both addition and multiplication as these are the two main operations are needed. If all of these axioms hold for the block size, then it is guaranteed, that the provided irreducible polynomial is correct for the specific block size. For this reason, every block size must be tested separately. Additionally, various helper functions, such as converting array of bits to decimal expression and vice versa is tested as these two functions are used intensely throughout the program.

\subsection{Choosing the exponents}
After implementing and testing that it was done correctly, it is time to compare the ciphers of different exponents to choose the most interesting ones for further analysis. First, the time it takes for the ciphers to encrypt some amount of plaintexts was compared based on the exponent. Four different block sizes were chosen (17, 33, 61 and 125). Then a test is: build a cipher with an exponent ranging from 2 to 40 with random key and round constants. Then count the amount of time it takes for the cipher to encrypt 1000 random plaintexts. This test is repeated 1000 times for each cipher, yielding that each function $x^e$ encrypts $1000*1000=1\,000\,000$ plaintexts. The results of this is given in the graphs \ref{fig:enc-exponent}. 

% \begin{figure}
%     \hspace{-1cm}
%     \centering
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/EncTimeExponent}
%     \end{minipage}\hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/EncTimeExponentLog}
%     \end{minipage}

%     \hspace{-1cm}
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/RoundNumberExponent}
%     \end{minipage}\hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/RoundNumberExponentLog}
%     \end{minipage}
%     \caption{Encryption and number of rounds based on exponent.}
%     \label{fig:enc-exponent}
% \end{figure}

The top graphs show the encryption test results, while the bottom graphs are the round number functions $R = \left\lceil n\log_e(2) \right\rceil$, where $e$ is the exponent and $n$ is the block size. The graphs on the right are just the graphs on the left with a logarithmic scale for better visualisation. These graphs showcase a few interesting things. First, it is evident, that as the exponent increases, the round number decreases and the amount of time it takes for the test also decreases. This is pretty obvious, as the number of rounds is lower, there are fewer computations. However, looking at the logarithmic scale, it is evident, that as the exponent is increased further, the time it takes to encrypt does not decrease much further. Hence, exponents $> 30$ do not bring further optimisations and we chose $x^{24}$ for further analysis.

Another interesting thing to note is that there are these dips in time which are evident in the top left graph. These sudden dips are for $x^4$, $x^8$ and all $x^{2^a}$ (the powers of two). This is, however, a consequence of the implementation. The cause of this is the square and multiply algorithm \ref{alg:exp}. This algorithm reaches its best case when exponents are the powers of two because then there zero multiplications and the number is only doubled. Thus, we choose $x^{16}$ for further analysis.

% \begin{figure}
%     \hspace{-1cm}
%     \centering
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/EncTimeBlock}
%     \end{minipage}\hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \include{Resources/RoundNumberBlock}
%     \end{minipage}
%     \caption{Encryption and number of rounds based on block size.}
%     \label{fig:enc-block}
% \end{figure}

Another interesting test we can perform is when the dependencies are flipped. That is, encryption time dependency on the block size. In this case, we choose $x^3$ and $x^5$ to check how they differ, together with previously chosen $x^{16}$ and $x^{24}$. Then the test is the same, where 1000 different ciphers are built with random key and round constants (essentially a random seed) and the time it takes for them to encrypt 1000 random plaintexts is counted. Each implemented block size (5, 8, 11, 17, 25, 31, 33, 47, 61, 83, 101, 125 and 127) is tested and the results of this is provided in the graph \ref{fig:enc-block}. This gives a great visualisation, that even though number of rounds is a logarithmic function, the encryption time grows exponentially. Another interesting detail is that as the exponent increases, the encryption time decreases, however, $x^{16}$ slightly faster than $x^{24}$. This is, as stated before, because of the square and multiply algorithm \ref{alg:exp} efficiency.

\textcolor{red}{Nezinau, kazkaip ar tikrai gerai? Gal labiau argumentuot, kodel buten tuos exponent pasirinkta}

\subsection{Generating test data}
As discussed in chapter \ref{chapter:prng} about statistical test suites, a sample space needs to be provided. Here it is discussed more broadly how these inputs were generated.

With NIST STS, generating the input files is simple. A utility CLI command \verb|generate-test-samples| was created which simply loops through the numbers from 0 up to some specified $t$. Then encrypts them with the given cipher and prints the result to the standard output. For this, a shell script \textit{generate-numbers-list.sh} was created which calls the CLI with the predefined cipher and its parameters and pipes the result to a file. The parameters are the 127 bit key and round constants which represent a random seed. For every different function $x^e$, the same seed was provided. The only difference was the number of round constants, as these represent the number of rounds.

Providing input for dieharder statistical test suite, things are a bit more complicated. Initially, the same setup was used, however, then the test files were too small and were rewound up to 7800 times which caused inaccurate results. For this reason, another method to provide the sample space was needed. Besides the input file, an endless bit stream can also be provided to the dieharder test suite. This stream must be of 32 bit numbers generated by the pseudo-random number generator being tested. For this, \verb|start-bit-stream| CLI command was created which does exactly that. The issue is, however, that some exponents do not work with 32 bit block size (i.e. $x^3$) because the function is not a permutation polynomial. To circumvent this, 33 bit block size is used and then one bit is dropped when outputting the result to the standard output. Another potential issue is that the test take a long time to finish and thus, when iterating over the sequence of bits, the iterator of the sequence might overflow. To prevent the sequence iterator from overflowing, it is reduced by a modulo $p = 4531145293$. This number is chosen specially to be a prime larger than $2^{32}$ and less than $2^{33}$ which means that the iterator forms a ring $\mod p$. In this case the numbers do eventually start to repeat, however the statistical test suite uses less than $2^{32}$ inputs, thus, this does not affect the test results. Similarly as before, shell scripts were created which contain the sample seed. Moreover, another shell script was created to run some of the experiments in parallel as some of them took more than 8 hours.

% Sudeti nedadetus references:
% galois fiels allow fast computational implementations (page 5, page 7)
% Fast exponentation (page 9)
% Best performed exponents (page 13)
% Generating NIST input file (page 15)
% Generating dieharder input (page 15)


% ------------- APPLICATIONS ----------------
\section{Applications (MPC, ZK, SNARK)}
\begin{itemize}
    \item Talk about the importance of ZK, SNARK and it's usage in block chain for example or evoting: \href{https://ethereum.org/en/zero-knowledge-proofs/}{URL} has great sources to read more
    \item Maybe talk about whether they're still available for MPC, ZK applications? What else?
\end{itemize}


% ------------- CONCLUSIONS ----------------
\section{Conclusion}
\begin{itemize}
    \item Summary of what has been achieved and what hasn't. Iterate again where these ciphers could be used, were they faster.
    \item These test suites fail to reject the null hypothesis giving more confidence that these ciphers produce a uniformly distributed collection of numbers.   It is evident that even the highly sophisticated tests cannot determine and bias. Thus, the ciphers produce non linear results from a linear input and behave like a pseudo-random number generator.
    % Further optimise with hardware specific AES yra, bet mes to negalim.
\end{itemize}


\setlength{\labelsep}{10pt} % Adds horizontal spacing between numbers and reference [X]<->Name...
\begin{thebibliography}{10}
\bibitem{MiMC} Martin R. Albrecht, Lorenzo Grassi, Christian Rechberger, Arnab Roy, Tyge Tiessen. \textit{MiMC: Efficient Encryption and Cryptographic Hashing with Minimal Multiplicative Complexity}, \href{https://eprint.iacr.org/2016/492.pdf}{URL}, 2016.

\bibitem{Poseidon} Lorenzo Grassi, Dmitry Khovratovich, Christian Rechberger, Arnab Roy, Markus Schofnegger. \textit{POSEIDON: A New Hash Function for Zero-Knowledge Proof Systems}, \href{https://www.usenix.org/system/files/sec21-grassi.pdf}{URL}, August 2021

\bibitem{GMiMC} Martin R. Albrecht, Lorenzo Grassi, Léo Perrin, Sebastian Ramacher, Christian Rechberger, Dragos Rotaru, Arnab Roy, Markus Schofnegger. \textit{Feistel Structures for MPC, and More}, \href{https://eprint.iacr.org/2019/397.pdf}{URL}, 2019.

\bibitem{Griffin} Lorenzo Grassi, Yonglin Hao, Christian Rechberger, Markus Schofnegger, Roman Walch, Qingju Wang. \textit{Horst Meets Fluid -SPN: Griffin for Zero-Knowledge Applications}, \href{https://eprint.iacr.org/2022/403.pdf}{URL}, 2022.

\bibitem{KNCipher} Kaisa Nyberg, Lars Ramkilde Knudsen. \textit{Provable security against a differential attack}, \href{https://link.springer.com/article/10.1007/BF00204800}{URL}, December 1995.

\bibitem{AESNI} Eslam G. AbdAllah, Yu Rang Kuang, Changcheng Huang. \textit{Advanced Encryption Standard New Instructions (AES-NI) Analysis: Security, Performance, and Power Consumption}, \href{https://www.sce.carleton.ca/faculty/huang/iccae-2020.pdf}{URL}, February 2020.

\bibitem{AESInMPC} Benny Pinkas, Thomas Schneider, Nigel P. Smart, Stephen C. Williams. \textit{Secure Two-Party Computation is Practical}, \href{https://eprint.iacr.org/2009/314.pdf}{URL}, 2009.

\bibitem{ZKOrigin} Shafi Goldwasser, Silvio Micali, Charles Rackoff. \textit{The Knowledge Complexity of Interactive Proof Systems}, \href{https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Proof%20Systems/The_Knowledge_Complexity_Of_Interactive_Proof_Systems.pdf}{URL}, February 1989.

\bibitem{RNGZK} Mihir Bellare, Moti Yung. \textit{Certifying Permutations: Non-Interactive Zero-Knowledge Based on any Trapdoor Permutation}, \href{https://cseweb.ucsd.edu/~mihir/papers/cct.pdf}{URL}, 1992.

\bibitem{ZKProofSystemsBook} Jonathan Bootle, Andrea Cerulli, Pyros Chaidos, Jens Groth. \textit{Efficient Zero-Knowledge Proof Systems}, \href{https://link.springer.com/chapter/10.1007/978-3-319-43005-8_1}{URL}, August 2016.

\bibitem{NIZK} Manuel Blum, Paul Feldman, Silvio Micali. \textit{Non-interactive zero-knowledge and its applications}, \href{https://doi.org/10.1145/62212.62222}{URL}, January 1988.

\bibitem{BGNCipher} Dan Boneh, Eu-Jin Goh, Kobbi Nissim. \textit{Evaluating 2-DNF Formulas on Ciphertexts}, \href{https://crypto.stanford.edu/~dabo/papers/2dnf.pdf}{URL}, April 2006.

\bibitem{FHEPHD} Craig Gentry. \textit{Fully Homomorphic Encryption Using Ideal Lattices}, \href{https://www.cs.cmu.edu/~odonnell/hits09/gentry-homomorphic-encryption.pdf}{URL}, May 2009.

\bibitem{FHEMention} Ronald L. Rivest, Len Adleman, Michael L. Dertouzos. \textit{On data banks and privacy homomorphisms}, \href{https://luca-giuzzi.unibs.it/corsi/Support/papers-cryptography/RAD78.pdf}{URL}, October 1978.

\bibitem{FHEImprovement1} Craig Gentry Amit Sahai Brent Waters. \textit{Homomorphic Encryption from Learning with Errors: Conceptually-Simpler, Asymptotically-Faster, Attribute-Based}, \href{https://eprint.iacr.org/2013/340.pdf}{URL}, June 2013.

\bibitem{FHEImprovement2} Jung Hee Cheon, Andrey Kim, Miran Kim, Yongsoo Song. \textit{Homomorphic Encryption for Arithmetic of Approximate Numbers}, \href{https://link.springer.com/chapter/10.1007/978-3-319-70694-8_15}{URL}, November 2017.

\bibitem{UntrustedWorkers} Rosario Gennaro, Craig Gentry, Bryan Parno. \textit{Non-Interactive Verifiable Computing: Outsourcing Computation to Untrusted Workers}, \href{https://eprint.iacr.org/2009/547.pdf}{URL}, February 2010.

\bibitem{Pinocchio} Bryan Parno, Jon Howell, Craig Gentry, Mariana Raykova. \textit{Pinocchio: Nearly Practical Verifiable Computation}, \href{https://eprint.iacr.org/2013/279.pdf}{URL}, May 2013.

\bibitem{Gepetto} Craig Costellom Cédric Fournet, Jon Howell, Markulf Kohlweiss, Benjamin Kreuter, Michael Naehrig, Bryan Parno, Samee Zahur. \textit{Geppetto: Versatile Verifiable Computation}, \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7163030}{URL}, May 2015.

\bibitem{Mirage} Ahmed Kosba, Dimitrios Papadopoulos, Charalampos Papamanthou, Dawn Song. \textit{MIRAGE: Succinct Arguments for Randomized Algorithms with Applications to Universal zk-SNARKs}, \href{https://eprint.iacr.org/2020/278.pdf}{URL}, March 2020.

\bibitem{Buffet} Riad S. Wahby, Srinath Setty, Zuocheng Ren, Andrew J. Blumberg, Michael Walfish. \textit{Efficient RAM and Control Flow in Verifiable Outsourced Computation}, \href{https://www.ndss-symposium.org/wp-content/uploads/2017/09/07_3_2.pdf}{URL}, February 2015.

\bibitem{STARK} Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, Michael Riabzev. \textit{Scalable, transparent, and post-quantum secure computational integrity}, \href{https://eprint.iacr.org/2018/046.pdf}{URL}, March 2018.

\bibitem{zcash} Eli Ben-Sasson, Alessandro Chiesa, Christina Garman, Matthew Green, Ian Miers, Eran Tromer, Madar Virza. \textit{Zerocash: Decentralized Anonymous Payments from Bitcoin}, \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6956581}{URL}, November 2014.

\bibitem{CloudTrend} Gartner. \textit{Gartner Says Cloud Will Become a Business Necessity by 2028}, \href{https://www.gartner.com/en/newsroom/press-releases/2023-11-29-gartner-says-cloud-will-become-a-business-necessity-by-2028}{URL}, November 2023.

\bibitem{MPCFirst} Oded Goldreich, Silvio Micali, Avi Wigderson. \textit{HOW TO PLAY ANY MENTAL GAME}, \href{https://dl.acm.org/doi/pdf/10.1145/28395.28420}{URL}, January 1987.

\bibitem{Estija} Dan Bogdanov, Liina Kamm, Baldur Kubo, Reimo Rebane, Ville Sokk, Riivo Talviste. \textit{Students and Taxes: a Privacy-Preserving Social Study Using Secure Computation}, \href{https://eprint.iacr.org/2015/1159.pdf}{URL}, December 2015.

\bibitem{Sharemind} \href{https://sharemind.cyber.ee}{Sharemind}.

\bibitem{Shamir} Adi Shamir. \textit{How to share a secret}, \href{https://dl.acm.org/doi/pdf/10.1145/359168.359176}{URL}, November 1979.

\bibitem{RINGLWE} Vadim Lyubashevsky, Chris Peikert, Oded Regev. \textit{On Ideal Lattices and Learning with Errors Over Rings}, \href{https://eprint.iacr.org/2012/230.pdf}{URL}, June 2013.

\bibitem{CKKS} Jung Hee Cheon, Andrey Kim, Miran Kim, Yongsoo Song. \textit{Homomorphic Encryption for Arithmetic of Approximate Numbers}, \href{https://eprint.iacr.org/2016/421.pdf}{URL}, May 2016.

\bibitem{BFV1} Zvika Brakerski. \textit{Fully Homomorphic Encryption without Modulus Switching from Classical GapSVP}, \href{https://eprint.iacr.org/2012/078.pdf}{URL}, February 2012.

\bibitem{BFV2} Junfeng Fan, Frederik Vercauteren. \textit{Somewhat Practical Fully Homomorphic Encryption}, \href{https://eprint.iacr.org/2012/144.pdf}{URL}, March 2012.

\bibitem{BFVImprov} Beatrice Biasioli, Chiara Marcolla, Marco Calderini, Johannes Mono. \textit{Improving and Automating BFV Parameters Selection: An Average-Case Approach}, \href{https://eprint.iacr.org/2023/600.pdf}{URL}, April 2023.

\bibitem{Rescue} Abdelrahaman Aly, Tomer Ashur, Eli Ben-Sasson, Siemen Dhooghe, Alan Szepieniec. \textit{Design of Symmetric-Key Primitives for Advanced Cryptographic Protocols}, \href{https://eprint.iacr.org/2019/426.pdf}{URL}, April 2019.

\bibitem{CryptanalysisBook} Gregory V. Bard. \textit{Algebraic Cryptanalysis}, \href{https://link.springer.com/book/10.1007/978-0-387-88757-9}{URL}, 2009.

\bibitem{FieldNotes} David Forney. \textit{Introduction to finite fields}, \href{http://web.stanford.edu/~marykw/classes/CS250_W19/readings/Forney_Introduction_to_Finite_Fields.pdf}{URL}, September 2016.

\bibitem{PPIntro} Christopher J. Shallue. \textit{Permutation Polynomials of Finite Fields}, \href{https://arxiv.org/pdf/1211.6044.pdf}{URL}, May 2012.

\bibitem{CipherGraphs} J\'er\'emy Jean. \textit{TikZ for Cryptographers}, \href{https://www.iacr.org/authors/tikz/}{URL}, 2016.

\bibitem{DESBreak} Eli Biham, Adi Shamir. \textit{Differential Cryptanalysis of DES-like Cryptosystems}, \href{https://link.springer.com/content/pdf/10.1007/3-540-38424-3_1.pdf}{URL}, January 1991.

\bibitem{OneTime} Claude E. Shannon. \textit{Communication Theory of Secrecy Systems}, \href{https://pages.cs.wisc.edu/~rist/642-spring-2014/shannon-secrecy.pdf}{URL}, October 1949.

\bibitem{DESLinear} Mitsuru Matsui. \textit{Linear Cryptanalysis Method for DES Cipher}, \href{https://link.springer.com/content/pdf/10.1007/3-540-48285-7_33.pdf}{URL}, 1993.

\bibitem{InterpolationAttack} Thomas Jakobsen, Lars R. Knudsen. \textit{The interpolation attack on block ciphers}, \href{https://link.springer.com/chapter/10.1007/BFb0052332}{URL}, February 1996.

\bibitem{HigherOrderAttack1} Lars R. Knudsen. \textit{Truncated and higher order differentials}, \href{https://link.springer.com/chapter/10.1007/3-540-60590-8_16}{URL}, 1994.

\bibitem{HigherOrderAttack2} Xuejia Lai. \textit{Higher Order Derivatives and Differential Cryptanalysis}, \href{https://link.springer.com/chapter/10.1007/978-1-4615-2694-0_23}{URL}, 1994.

\bibitem{MiMCAttack} Maria Eichlseder, Lorenzo Grassi, Reinhard Lüftenegger, Morten Øygarden, Christian Rechberger, Markus Schofnegger, Qingju Wang. \textit{An Algebraic Attack on Ciphers with Low-Degree Round Functions: Application to Full MiMC}, \href{https://eprint.iacr.org/2020/182.pdf}{URL}, February 2020.

\bibitem{CryptanalysisCource} Maria Eichlseder, Marcel Nageler. \textit{CRYPTANALYSIS}, \href{https://www.iaik.tugraz.at/course/cryptanalysis-705068-sommersemester-2024}{URL}, 2024.

\bibitem{QuantumZK} Cheng-Long Li, Kai-Yi Zhang, Xingjian Zhang, Kui-Xing Yang, Yu Han, Su-Yi Cheng, Hongrui Cui, Wen-Zhao Liu, Ming-Han Li, Yang Liu, Bing Bai, Hai-Hao Dong, Jun Zhang, Xiongfeng Ma, Yu Yu, Jingyun Fan, Qiang Zhang, Jian-Wei Pan. \textit{Device-independent quantum randomness–enhanced zero-knowledge proof}, \href{https://doi.org/10.1073/pnas.2205463120}{URL}, November 2023.

\bibitem{LatticeRNG} TODO: https://ieeexplore.ieee.org/abstract/document/9870587/

\bibitem{NIST} Andrew Rukhin, Juan Soto, James Nechvatal, Miles Smid, Elaine Barker, Stefan Leigh, Mark Levenson, Mark Vangel, David Banks,Alan Heckert, James Dray, San Vo. \textit{A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications}, \href{https://csrc.nist.gov/pubs/sp/800/22/r1/upd1/final}{URL}, April 2010.

\bibitem{Dieharder} Robert G. Brown. \textit{DieHarder: A Gnu Public License Random Number Tester}, \href{https://rurban.github.io/dieharder/manual/dieharder.pdf}{URL}, 2006.

\bibitem{Diehard} George Marsaglia. \textit{Diehard Battery of Tests of Randomness}, \href{https://web.archive.org/web/20160125103112/http://stat.fsu.edu/pub/diehard/}{URL}, 1995.

\end{thebibliography}

\end{document}
